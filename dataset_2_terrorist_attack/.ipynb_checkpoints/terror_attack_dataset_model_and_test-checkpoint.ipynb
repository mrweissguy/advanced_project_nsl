{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural graph learning on terrorist attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intorduction to dataset\n",
    "\n",
    "The [Terrorist attack dataset]() consists of 1293 terrorist attacks. Each terrorist attach as xx features, represented as a vector 0/1 describing the absence/presence of a feature. Each terrorist attack is assigned a label describing the type of the terrorist attack. There is a total of 6 labels.\n",
    "The dataset also cosists of two possible graph structures - one where the nodes are attacks and linked if they share location, and another where links are formed by shared locations and shared terrorist oraganisation. ?? Skriv om..\n",
    "\n",
    "one based on co-located attacks and another, based on co-located attacks organized by the same terrorist organization\n",
    "\n",
    "The dataset is a subset of the original dataset. For related papers on the same dataset, see [Bin Zhao, et. al. \"Entity and Relationship Labeling in Affiliation Networks.\" ICML. 2006.](https://linqspub.soe.ucsc.edu/basilic/web/Publications/2006/zhao:sna06/zhaosna06.pdf)\n",
    "\n",
    "For the preprocessing the id's were shortened and seperated exports as tsv files. The notebook for preprocessing can be found [here](preprocessing_terrorist_attack_dataset.ipynb). \n",
    "\n",
    "Dataset summary:\n",
    "* 1293 terrorist attacks.\n",
    "* 106 distinct features.\n",
    "* Labels for classifying the terrorist attacks:\n",
    "    * Arson\n",
    "    * Bombing\n",
    "    * Kidnapping\n",
    "    * NBCR_Attack\n",
    "    * Weapon_Attack\n",
    "    * other_attack\n",
    "* Each terrorist attack has one label.\n",
    "\n",
    "## Experiment\n",
    "\n",
    "The goal is to correctly classify the terrorist attakcks and examine the performance difference between the Neural Graph Learning model and a base model.\n",
    "\n",
    "To proberly examine the performance difference between the models, each model is trainined with traning sizes from 0.1 to 0.85 with 0.5 incrementes. The models is run 5 times at each traning sizes whereafter the average results are presented in a graph.\n",
    "\n",
    "## References\n",
    "\n",
    "Large parts of the code for preprocessing, loading train, test and validation data, evaluation, generation of Keras functional models is modefied from Tensorflows tutorials and resources introducing neural structed learning. The original code can be found here: [Tensorflows github](https://github.com/tensorflow/neural-structured-learning) and [Guide and Tutorials](https://www.tensorflow.org/neural_structured_learning/framework).\n",
    "Furthermore, additional information about the API can be found [here](https://www.tensorflow.org/neural_structured_learning/api_docs/python/nsl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet neural-structured-learning\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import neural_structured_learning as nsl\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x terrorist_attack_loc.edges\r\n",
      "x terrorist_attack.nodes\r\n",
      "x terrorist_attack_loc_org.edges\r\n"
     ]
    }
   ],
   "source": [
    "!tar -C /tmp -xvzf /Users/johanweisshansen/Documents/DTU/3.semester/advanced_project/oticon_project/terror_data/nsl/dataset_test_4/TerrorAttackNew/Archive2.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams(object):\n",
    "  \"\"\"Hyperparameters used for training.\"\"\"\n",
    "  def __init__(self):\n",
    "    ### dataset parameters\n",
    "    self.num_classes = 6\n",
    "    self.max_seq_length = 106\n",
    "    ### neural graph learning parameters\n",
    "    self.distance_type = nsl.configs.DistanceType.L2\n",
    "    self.graph_regularization_multiplier = 0.3\n",
    "    self.num_neighbors = 1\n",
    "    ### model architecture\n",
    "    self.num_fc_units = [50,50]\n",
    "    ### training parameters\n",
    "    self.train_epochs = 50\n",
    "    self.batch_size = 50\n",
    "    self.dropout_rate = 0.5\n",
    "    ### eval parameters\n",
    "    self.eval_steps = None  # All instances in the test set are evaluated.\n",
    "\n",
    "HPARAMS = HParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train, valid and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_example(example_proto):\n",
    "\n",
    "    feature_spec = {\n",
    "        'attributes':\n",
    "            tf.io.FixedLenFeature([HPARAMS.max_seq_length],\n",
    "                                tf.int64,\n",
    "                                default_value=tf.constant(\n",
    "                                    0,\n",
    "                                    dtype=tf.int64,\n",
    "                                    shape=[HPARAMS.max_seq_length])),\n",
    "        'label':\n",
    "          tf.io.FixedLenFeature((), tf.int64, default_value=-1),\n",
    "    }\n",
    "    # We also extract corresponding neighbor features in a similar manner to\n",
    "  # the features above.\n",
    "    for i in range(HPARAMS.num_neighbors):\n",
    "        nbr_feature_key = '{}{}_{}'.format('NL_nbr_', i, 'attributes')\n",
    "        nbr_weight_key = '{}{}{}'.format('NL_nbr_', i, '_weight')\n",
    "        feature_spec[nbr_feature_key] = tf.io.FixedLenFeature(\n",
    "            [HPARAMS.max_seq_length],\n",
    "            tf.int64,\n",
    "            default_value=tf.constant(\n",
    "            0, dtype=tf.int64, shape=[HPARAMS.max_seq_length]))\n",
    "\n",
    "    # We assign a default value of 0.0 for the neighbor weight so that\n",
    "    # graph regularization is done on samples based on their exact number\n",
    "    # of neighbors. In other words, non-existent neighbors are discounted.\n",
    "    feature_spec[nbr_weight_key] = tf.io.FixedLenFeature(\n",
    "        [1], tf.float32, default_value=tf.constant([0.0]))\n",
    "\n",
    "    features = tf.io.parse_single_example(example_proto, feature_spec)\n",
    "\n",
    "    labels = features.pop('label')\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def make_dataset(file_path, training=False):\n",
    "  #Creates a `tf.data.TFRecordDataset`.\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset([file_path])\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(10000)\n",
    "    dataset = dataset.map(parse_example)\n",
    "    dataset = dataset.batch(HPARAMS.batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def functional_model(hparams):\n",
    "    \"\"\"Creates a functional API-based multi-layer perceptron model.\"\"\"\n",
    "    inputs = tf.keras.Input(shape=(hparams.max_seq_length,), dtype='int64', name='attributes')\n",
    "\n",
    "  # casting one hot to floating point format.\n",
    "    cur_layer = tf.keras.layers.Lambda(\n",
    "      lambda x: tf.keras.backend.cast(x, tf.float32))(\n",
    "          inputs)\n",
    "\n",
    "    for num_units in hparams.num_fc_units:\n",
    "        cur_layer = tf.keras.layers.Dense(num_units, activation='relu')(cur_layer)\n",
    "        cur_layer = tf.keras.layers.Dropout(hparams.dropout_rate)(cur_layer)\n",
    "#         cur_layer = tf.keras.layers.BatchNormalization()(cur_layer)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "      hparams.num_classes, activation='softmax')(\n",
    "          cur_layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to print evaluation metrics.\n",
    "def print_metrics(model_desc, eval_metrics):\n",
    "    print('\\n')\n",
    "    print('Eval accuracy for ', model_desc, ': ', eval_metrics['accuracy'])\n",
    "#     print('Eval loss for ', model_desc, ': ', eval_metrics['loss'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for training base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traning_base_model(train_dataset, valid_dataset):\n",
    "    base_model = functional_model(HPARAMS)\n",
    "    # Compile and train the base MLP model\n",
    "    base_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    base_model_history = base_model.fit(train_dataset, epochs=HPARAMS.train_epochs, verbose=0, validation_data=valid_dataset)\n",
    "    \n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for training graph model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_graph_model(train_dataset, valid_dataset):\n",
    "    # Build a new base MLP model.\n",
    "    base_reg_model = functional_model(HPARAMS)\n",
    "    \n",
    "    # Wrap the base MLP model with graph regularization.\n",
    "    graph_reg_config = nsl.configs.make_graph_reg_config(\n",
    "        max_neighbors=HPARAMS.num_neighbors,\n",
    "        multiplier=HPARAMS.graph_regularization_multiplier,\n",
    "        distance_type=HPARAMS.distance_type,\n",
    "        sum_over_axis=-1)\n",
    "    graph_reg_model = nsl.keras.GraphRegularization(base_reg_model, graph_reg_config)\n",
    "    \n",
    "    graph_reg_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    graph_reg_history = graph_reg_model.fit(train_dataset, epochs=HPARAMS.train_epochs, verbose=0, validation_data=valid_dataset)\n",
    "    \n",
    "    return graph_reg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for generating train, test and validataion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTrainingData(train_percent):\n",
    "    # getting the training percentages\n",
    "    !python preprocess_terror_attack_dataset.py \\\n",
    "    --input_content=/tmp/terrorist_attack.nodes \\\n",
    "    --input_graph=/tmp/terrorist_attack_loc_org.edges \\\n",
    "    --max_nbrs=15 \\\n",
    "    --train_percentage=$train_percent\\\n",
    "    --output_train_data=/tmp/train_merged_examples.tfr \\\n",
    "    --output_test_data=/tmp/test_examples.tfr \\\n",
    "    --output_valid_data=/tmp/valid_examples.tfr \\\n",
    "    \n",
    "        # generating train and test data\n",
    "    train_dataset = make_dataset('/tmp/train_merged_examples.tfr', training=True)\n",
    "    test_dataset = make_dataset('/tmp/test_examples.tfr')\n",
    "    valid_dataset = make_dataset('/tmp/valid_examples.tfr')\n",
    "    \n",
    "    return train_dataset, test_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating over different training sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# defining the training size we need to iterate over\n",
    "train_percentage = []\n",
    "train_percentage.append(0.01) # starting the list at 1% of training data\n",
    "\n",
    "for i in np.arange(0.05, 0.9, 0.05):\n",
    "    train_percentage.append(round(i,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- iteration:  1 ------------------------\n",
      "---------------------training at percentage  0.01 --------------------------------\n",
      "preprocess_terror_attack_dataset.py:132: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(in_file, 'rU') as cora_content:\n",
      "Reading graph file: /tmp/terrorist_attack_loc_org.edges...\n",
      "Done reading 571 edges from: /tmp/terrorist_attack_loc_org.edges (0.00 seconds).\n",
      "Making all edges bi-directional...\n",
      "Done (0.00 seconds). Total graph nodes: 257\n",
      "Joining seed and neighbor tf.train.Examples with graph edges...\n",
      "Done creating and writing 15 merged tf.train.Examples (0.00 seconds).\n",
      "Out-degree histogram: [(0, 13), (2, 2)]\n",
      "Output training data written to TFRecord file: /tmp/train_merged_examples.tfr.\n",
      "Output test data written to TFRecord file: /tmp/test_examples.tfr.\n",
      "Output valid data written to TFRecord file: /tmp/valid_examples.tfr.\n",
      "Total running time: 0.00 minutes.\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.1352 - accuracy: 0.5804\n",
      "Eval accuracy for  Base MLP model :  0.5803723\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2395 - accuracy: 0.4281 - graph_loss: 0.0000e+00\n",
      "Eval accuracy for  MLP + graph regularization :  0.42808798\n",
      "---------------------training at percentage  0.05 --------------------------------\n",
      "preprocess_terror_attack_dataset.py:132: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(in_file, 'rU') as cora_content:\n",
      "Reading graph file: /tmp/terrorist_attack_loc_org.edges...\n",
      "Done reading 571 edges from: /tmp/terrorist_attack_loc_org.edges (0.00 seconds).\n",
      "Making all edges bi-directional...\n",
      "Done (0.00 seconds). Total graph nodes: 257\n",
      "Joining seed and neighbor tf.train.Examples with graph edges...\n",
      "Done creating and writing 65 merged tf.train.Examples (0.00 seconds).\n",
      "Out-degree histogram: [(0, 56), (2, 2), (3, 3), (4, 2), (5, 1), (7, 1)]\n",
      "Output training data written to TFRecord file: /tmp/train_merged_examples.tfr.\n",
      "Output test data written to TFRecord file: /tmp/test_examples.tfr.\n",
      "Output valid data written to TFRecord file: /tmp/valid_examples.tfr.\n",
      "Total running time: 0.00 minutes.\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.9311 - accuracy: 0.7038\n",
      "Eval accuracy for  Base MLP model :  0.70383275\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.9058 - accuracy: 0.7282 - graph_loss: 0.0000e+00\n",
      "Eval accuracy for  MLP + graph regularization :  0.728223\n",
      "---------------------training at percentage  0.1 --------------------------------\n",
      "preprocess_terror_attack_dataset.py:132: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(in_file, 'rU') as cora_content:\n",
      "Reading graph file: /tmp/terrorist_attack_loc_org.edges...\n",
      "Done reading 571 edges from: /tmp/terrorist_attack_loc_org.edges (0.00 seconds).\n",
      "Making all edges bi-directional...\n",
      "Done (0.00 seconds). Total graph nodes: 257\n",
      "Joining seed and neighbor tf.train.Examples with graph edges...\n",
      "Done creating and writing 134 merged tf.train.Examples (0.01 seconds).\n",
      "Out-degree histogram: [(0, 114), (1, 10), (2, 6), (4, 3), (5, 1)]\n",
      "Output training data written to TFRecord file: /tmp/train_merged_examples.tfr.\n",
      "Output test data written to TFRecord file: /tmp/test_examples.tfr.\n",
      "Output valid data written to TFRecord file: /tmp/valid_examples.tfr.\n",
      "Total running time: 0.00 minutes.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.5763 - accuracy: 0.8092\n",
      "Eval accuracy for  Base MLP model :  0.8091603\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.5635 - accuracy: 0.8454 - graph_loss: 0.0000e+00\n",
      "Eval accuracy for  MLP + graph regularization :  0.8454198\n",
      "---------------------training at percentage  0.15 --------------------------------\n",
      "preprocess_terror_attack_dataset.py:132: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(in_file, 'rU') as cora_content:\n",
      "Reading graph file: /tmp/terrorist_attack_loc_org.edges...\n",
      "Done reading 571 edges from: /tmp/terrorist_attack_loc_org.edges (0.00 seconds).\n",
      "Making all edges bi-directional...\n",
      "Done (0.00 seconds). Total graph nodes: 257\n",
      "Joining seed and neighbor tf.train.Examples with graph edges...\n",
      "Done creating and writing 203 merged tf.train.Examples (0.03 seconds).\n",
      "Out-degree histogram: [(0, 170), (1, 15), (2, 9), (3, 4), (4, 1), (5, 1), (6, 1), (7, 2)]\n",
      "Output training data written to TFRecord file: /tmp/train_merged_examples.tfr.\n",
      "Output test data written to TFRecord file: /tmp/test_examples.tfr.\n",
      "Output valid data written to TFRecord file: /tmp/valid_examples.tfr.\n",
      "Total running time: 0.00 minutes.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.4470 - accuracy: 0.8708\n",
      "Eval accuracy for  Base MLP model :  0.87077534\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.4796 - accuracy: 0.8668 - graph_loss: 0.0000e+00\n",
      "Eval accuracy for  MLP + graph regularization :  0.8667992\n",
      "---------------------training at percentage  0.2 --------------------------------\n",
      "preprocess_terror_attack_dataset.py:132: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(in_file, 'rU') as cora_content:\n",
      "Reading graph file: /tmp/terrorist_attack_loc_org.edges...\n",
      "Done reading 571 edges from: /tmp/terrorist_attack_loc_org.edges (0.00 seconds).\n",
      "Making all edges bi-directional...\n",
      "Done (0.00 seconds). Total graph nodes: 257\n",
      "Joining seed and neighbor tf.train.Examples with graph edges...\n",
      "Done creating and writing 228 merged tf.train.Examples (0.03 seconds).\n",
      "Out-degree histogram: [(0, 185), (1, 22), (2, 5), (3, 2), (4, 4), (5, 2), (6, 1), (7, 3), (8, 4)]\n",
      "Output training data written to TFRecord file: /tmp/train_merged_examples.tfr.\n",
      "Output test data written to TFRecord file: /tmp/test_examples.tfr.\n",
      "Output valid data written to TFRecord file: /tmp/valid_examples.tfr.\n",
      "Total running time: 0.00 minutes.\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4699 - accuracy: 0.8593\n",
      "Eval accuracy for  Base MLP model :  0.8593449\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 0.4682 - accuracy: 0.8728 - graph_loss: 0.0000e+00\n",
      "Eval accuracy for  MLP + graph regularization :  0.87283236\n",
      "---------------------training at percentage  0.25 --------------------------------\n",
      "preprocess_terror_attack_dataset.py:132: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(in_file, 'rU') as cora_content:\n",
      "Reading graph file: /tmp/terrorist_attack_loc_org.edges...\n",
      "Done reading 571 edges from: /tmp/terrorist_attack_loc_org.edges (0.00 seconds).\n",
      "Making all edges bi-directional...\n",
      "Done (0.00 seconds). Total graph nodes: 257\n",
      "Joining seed and neighbor tf.train.Examples with graph edges...\n",
      "Done creating and writing 300 merged tf.train.Examples (0.02 seconds).\n",
      "Out-degree histogram: [(0, 256), (1, 21), (2, 9), (3, 5), (6, 3), (8, 4), (9, 2)]\n",
      "Output training data written to TFRecord file: /tmp/train_merged_examples.tfr.\n",
      "Output test data written to TFRecord file: /tmp/test_examples.tfr.\n",
      "Output valid data written to TFRecord file: /tmp/valid_examples.tfr.\n",
      "Total running time: 0.00 minutes.\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4396 - accuracy: 0.8880\n",
      "Eval accuracy for  Base MLP model :  0.8879668\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.4649 - accuracy: 0.8880 - graph_loss: 0.0000e+00\n",
      "Eval accuracy for  MLP + graph regularization :  0.8879668\n",
      "---------------------training at percentage  0.3 --------------------------------\n",
      "^C\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-31357a1dd2b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# creating and training the base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraning_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         eval_results_base_model = dict(\n",
      "\u001b[0;32m<ipython-input-8-f92737d8a455>\u001b[0m in \u001b[0;36mtraning_base_model\u001b[0;34m(train_dataset, valid_dataset)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         metrics=['accuracy'])\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mbase_model_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHPARAMS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m                       \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                       \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                       total_epochs=1)\n\u001b[0m\u001b[1;32m    371\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m    372\u001b[0m                                  prefix='val_')\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lists for holding results\n",
    "graph_accuracy_by_training_size_avg = []\n",
    "base_accuracy_by_training_size_avg = []\n",
    "\n",
    "for j in range(5):\n",
    "    \n",
    "    print(\"----------------------- iteration: \", j+1, \"------------------------\" )\n",
    "    base_model_results_list = []\n",
    "    graph_model_results_list = []\n",
    "    for i in range(len(train_percentage)):\n",
    "\n",
    "        print(\"---------------------training at percentage \", train_percentage[i], \"--------------------------------\")\n",
    "\n",
    "        # creating test and training data\n",
    "        train_dataset, test_dataset, valid_dataset = generateTrainingData(train_percentage[i])\n",
    "\n",
    "        # creating and training the base model\n",
    "        base_model = traning_base_model(train_dataset, valid_dataset)\n",
    "\n",
    "        eval_results_base_model = dict(\n",
    "        zip(base_model.metrics_names,\n",
    "            base_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)))\n",
    "        print_metrics('Base MLP model', eval_results_base_model)\n",
    "\n",
    "        base_model_results_list.append(eval_results_base_model)\n",
    "\n",
    "        # creating and training the graph model\n",
    "        graph_model = training_graph_model(train_dataset, valid_dataset)\n",
    "\n",
    "        eval_results_graph_regulated_model = dict(\n",
    "        zip(graph_model.metrics_names,\n",
    "            graph_model.evaluate(test_dataset, steps=HPARAMS.eval_steps)))\n",
    "        print_metrics('MLP + graph regularization', eval_results_graph_regulated_model)\n",
    "\n",
    "        graph_model_results_list.append(eval_results_graph_regulated_model)\n",
    "\n",
    "\n",
    "    graph_accuracy_by_training_size_avg.append(graph_model_results_list)\n",
    "    base_accuracy_by_training_size_avg.append(base_model_results_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Since we ran 5 iterations we first have to average the numner training accuracy at the different training sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4a836d42aea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbase_avg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_accuracy_by_training_size_avg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtmp_avg_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_accuracy_by_training_size_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "graph_avg_list = []\n",
    "base_avg_list = []\n",
    "\n",
    "for i in range(0,len(graph_accuracy_by_training_size_avg[0])):\n",
    "    tmp_avg_value = 0\n",
    "    for j in range(0,len(graph_accuracy_by_training_size_avg)):\n",
    "        tmp_avg_value += graph_accuracy_by_training_size_avg[j][i]['accuracy']\n",
    "    graph_avg_list.append(tmp_avg_value/5)\n",
    "    \n",
    "for i in range(0,len(base_accuracy_by_training_size_avg[0])):\n",
    "    tmp_avg_value = 0\n",
    "    for j in range(0,len(base_accuracy_by_training_size_avg)):\n",
    "        tmp_avg_value += base_accuracy_by_training_size_avg[j][i]['accuracy']\n",
    "    base_avg_list.append(tmp_avg_value/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get at better idea of the difference in learning at different training sizes we substract the two model preformances from each other.\n",
    "\n",
    "A postive value indicates a gain for the graph based model over the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_graph_and_basemodel = []\n",
    "\n",
    "for i in range(len(base_avg_list)):\n",
    "    diff_graph_and_basemodel.append(graph_avg_list[i]- base_avg_list[i])\n",
    "\n",
    "collected_list = []\n",
    "collected_list.append(base_avg_list)\n",
    "collected_list.append(graph_avg_list)\n",
    "collected_list.append(diff_graph_and_basemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "columns = train_percentage\n",
    "rows = ['base model', 'graph model', 'pref. diff.']\n",
    "\n",
    "# Get some pastel shades for the colors\n",
    "n_rows = len(collected_list)\n",
    "\n",
    "# Initialize the vertical-offset for the stacked bar chart.\n",
    "y_offset = np.zeros(len(train_percentage))\n",
    "\n",
    "# Plot bars and create text labels for the table\n",
    "cell_text = []\n",
    "for row in range(n_rows):\n",
    "    y_offset = collected_list[row]\n",
    "    cell_text.append(['%1.3f' % x for x in y_offset])\n",
    "\n",
    "# Add a table at the bottom of the axes\n",
    "table = plt.table(cellText=cell_text,\n",
    "                      rowLabels=rows,\n",
    "                      colLabels=columns,\n",
    "                      rowColours=['#5b9ac4', '#fd8e39', '#ffffff'],\n",
    "                      loc='bottom')\n",
    "\n",
    "table.set_fontsize(14)\n",
    "\n",
    "# table.auto_set_font_size(False)\n",
    "table.set_fontsize(14)\n",
    "table.scale(1, 1.7)\n",
    "# Adjust layout to make room for the table:\n",
    "plt.subplots_adjust(left=0.2, bottom=0.2)\n",
    "\n",
    "\n",
    "# plt.plot(graph_collected)\n",
    "plt.plot(base_avg_list)\n",
    "plt.plot(graph_avg_list)\n",
    "\n",
    "# plt.ylabel(\"Loss in ${0}'s\".format(value_increment))\n",
    "# plt.yticks(values * value_increment, ['%d' % val for val in values])\n",
    "plt.xticks([])\n",
    "plt.title('Comparison between base and graph model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "# plt.xlabel('training size')\n",
    "plt.annotate('training size', xy=(1,0), xytext=(-669, -3), ha='left', va='top',\n",
    "            xycoords='axes fraction', textcoords='offset points')\n",
    "plt.legend(['base model', 'graph reg model'], loc='upper left')\n",
    "plt.savefig('plots/terroist_attack_comparison.png', bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
