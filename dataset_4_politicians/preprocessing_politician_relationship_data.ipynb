{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of data - Dataset 4 - Politicians\n",
    "\n",
    "In this notebook the politician dataset, containing a [list of members](https://github.com/suneman/socialgraphs2018/blob/master/files/data_US_congress/H115.csv) of the 115th House of representatives and a [zip-file](https://github.com/suneman/socialgraphs2018/blob/master/files/data_twitter/tweets.zip) of the 200 most recent tweets of said members (collected around September 2018), is preprocessed in order to train neural network models on it. These files have been prepared and are hosted by Technical University of Denmark Associate Professor Sune Lehmann Jørgensen for the 2018 installment of his DTU Compute course 02805 Social Graphs and Interactions. This notebook adds further required extension and preprocessing for the purposes of this project. The dataset describes the relationship between [House representatives of the 115th US congress](https://en.wikipedia.org/wiki/List_of_members_of_the_United_States_House_of_Representatives_in_the_115th_Congress_by_seniority) and the purpose of the built machine learning algorithm is to classify politicians in terms of party.\n",
    "\n",
    "This notebook assumes the presence of the following files and file structure:\n",
    "\n",
    "- H115.csv\n",
    "- tweets (make sure to unzip the provided file)\n",
    "    - AustinScottGA08\n",
    "    - BennieGThompson\n",
    "    - ...\n",
    "- legislators-current.csv (file used to translate Twitter handles to Wiki IDs, acquired [here](https://github.com/unitedstates/congress-legislators/))\n",
    "\n",
    "This notebook adds the following files to the directory:\n",
    "\n",
    "- H115_nodes.csv\n",
    "- H115_connections.csv\n",
    "- H115_nodes_oh.tsv ('oh' for OneHot)\n",
    "- H115_connections.tsv\n",
    "\n",
    "The following steps is performed in this notebook:\n",
    "* Age (as of today's date) of the politicians is added as an extra feature. Sex has been handled manually.\n",
    "* Connections between the politicians are extracted based on retweets in Twitter data dumps.\n",
    "* Features are one-hot encoded.\n",
    "\n",
    "NB! Please note that Wikipedia page IDs or anything else could have been subject for change in the time between finishing this project and you viewing/executing this code. If that is the case, then you *will* get errors when executing the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features: Age and sex\n",
    "\n",
    "As an attempt to strengthen the generalization ability of the machine learning model, the original dataset containing `WikiPageName`, `State` and `Party` is extended with `Age` and `Sex`.\n",
    "\n",
    "`Sex` has been manually added to the `H115.csv` file one by one by looking up the politicians. <br>\n",
    "`Age` has been calculated and added, as done in the following code. Instances with names with problematic charactes have been manually added to the `H115.csv` file. <br>\n",
    "The result is the file `H115_vertices.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen # request urls\n",
    "from urllib import parse\n",
    "import re # regex\n",
    "import csv\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getinfobox(name):\n",
    "    baseurl = \"https://en.wikipedia.org/w/api.php/?\"\n",
    "    action = \"action=query\"\n",
    "    title = \"titles=\" + name\n",
    "    content = \"prop=revisions\"\n",
    "    rvprop =\"rvprop=timestamp|content\"\n",
    "    dataformat = \"format=json\"\n",
    "    section = \"rvsection=0\" # get only the infobox\n",
    "\n",
    "    query = \"%s%s&%s&%s&%s&%s&%s\" % (baseurl, action, title, content, rvprop, dataformat, section)\n",
    "    req = urlopen(query).read()\n",
    "    #print(\"ok\")\n",
    "    return req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsefeature(infoboxstring, feature):\n",
    "    #req = str(req)\n",
    "    infoboxstring = infoboxstring.decode(\"utf-8\")\n",
    "    \n",
    "    if feature == 'party':\n",
    "        out = re.findall(r'\\|party[\\ ]+= \\[\\[(\\w+)',infoboxstring)\n",
    "        out = sParty[0]\n",
    "    elif feature == 'bd':\n",
    "        out = re.findall(r'\\|[\\s]*birth_date[\\s]*=[\\s]*\\{\\{[B+b][irth date and age]*[irth based on age as of date]*[\\|]*[\\s]*[mf=yes]*[\\|]*[\\s]*([\\d]+)\\|([\\d]+)\\|([\\d]+)', infoboxstring) # regex\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_age(bornyear, bornmonth, bornday):\n",
    "    today = date.today()\n",
    "    return today.year - bornyear - ((today.month, today.day) < (bornmonth, bornday))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test function\n",
    "name = \"Nancy_Pelosi\"\n",
    "#name = 'Jim_Banks'\n",
    "res = parsefeature(getinfobox(name), 'bd')\n",
    "bornyear = int(res[0][0])\n",
    "bornmonth = int(res[0][1])\n",
    "bornday = int(res[0][2])\n",
    "calculate_age(bornyear, bornmonth, bornday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John_Conyers : 90\n",
      "Don_Young : 86\n",
      "Jim_Sensenbrenner : 76\n",
      "Hal_Rogers : 81\n",
      "Chris_Smith_(New_Jersey_politician) : 66\n",
      "Steny_Hoyer : 80\n",
      "Marcy_Kaptur : 73\n",
      "Sander_Levin : 88\n",
      "Joe_Barton : 70\n",
      "Pete_Visclosky : 70\n",
      "Peter_DeFazio : 72\n",
      "John_Lewis_(civil_rights_leader) : 79\n",
      "Louise_Slaughter : 90\n",
      "Lamar_Smith : 72\n",
      "Fred_Upton : 66\n",
      "Nancy_Pelosi : 79\n",
      "Jimmy_Duncan_(politician) : 72\n",
      "Frank_Pallone : 68\n",
      "Eliot_Engel : 72\n",
      "Nita_Lowey : 82\n",
      "Richard_Neal : 70\n",
      "Dana_Rohrabacher : 72\n",
      "Ileana_Ros-Lehtinen : 67\n",
      "José_E._Serrano : 67\n",
      "David_Price_(American_politician) : 79\n",
      "Rosa_DeLauro : 76\n",
      "Collin_Peterson : 75\n",
      "Maxine_Waters : 81\n",
      "Sam_Johnson : 89\n",
      "Jerry_Nadler : 72\n",
      "Jim_Cooper : 65\n",
      "Xavier_Becerra : 61\n",
      "Sanford_Bishop : 72\n",
      "Ken_Calvert : 66\n",
      "Jim_Clyburn : 79\n",
      "Anna_Eshoo : 76\n",
      "Bob_Goodlatte : 67\n",
      "Gene_Green : 72\n",
      "Luis_Gutiérrez : 72\n",
      "Alcee_Hastings : 83\n",
      "Eddie_Bernice_Johnson : 84\n",
      "Peter_T._King : 75\n",
      "Carolyn_Maloney : 73\n",
      "Lucille_Roybal-Allard : 78\n",
      "Ed_Royce : 68\n",
      "Bobby_Rush : 73\n",
      "Bobby_Scott_(politician) : 72\n",
      "Nydia_Velázquez : 72\n",
      "Bennie_Thompson : 71\n",
      "Frank_Lucas_(Oklahoma_politician) : 59\n",
      "Lloyd_Doggett : 73\n",
      "Mike_Doyle_(American_politician) : 66\n",
      "Rodney_Frelinghuysen : 73\n",
      "Sheila_Jackson_Lee : 69\n",
      "Walter_B._Jones_Jr. : 76\n",
      "Frank_LoBiondo : 73\n",
      "Zoe_Lofgren : 71\n",
      "Mac_Thornberry : 61\n",
      "Elijah_Cummings : 68\n",
      "Earl_Blumenauer : 71\n",
      "Robert_Aderholt : 54\n",
      "Kevin_Brady : 64\n",
      "Danny_K._Davis : 78\n",
      "Diana_DeGette : 62\n",
      "Kay_Granger : 76\n",
      "Ron_Kind : 56\n",
      "Jim_McGovern_(American_politician) : 60\n",
      "Bill_Pascrell : 82\n",
      "Pete_Sessions : 64\n",
      "Brad_Sherman : 65\n",
      "John_Shimkus : 61\n",
      "Adam_Smith_(politician) : 54\n",
      "Gregory_Meeks : 66\n",
      "Barbara_Lee : 73\n",
      "Bob_Brady : 74\n",
      "Steve_Chabot : 66\n",
      "Mike_Capuano : 67\n",
      "Joe_Crowley : 57\n",
      "John_B._Larson : 71\n",
      "Grace_Napolitano : 83\n",
      "Paul_Ryan : 49\n",
      "Jan_Schakowsky : 75\n",
      "Mike_Simpson : 69\n",
      "Mike_Thompson_(California_politician) : 68\n",
      "Greg_Walden : 62\n",
      "Lacy_Clay : 63\n",
      "John_Culberson : 63\n",
      "Susan_Davis_(politician) : 75\n",
      "Sam_Graves : 56\n",
      "Darrell_Issa : 66\n",
      "James_Langevin : 55\n",
      "Rick_Larsen : 54\n",
      "Betty_McCollum : 65\n",
      "Adam_Schiff : 59\n",
      "Pat_Tiberi : 57\n",
      "Bill_Shuster : 58\n",
      "Stephen_F._Lynch : 64\n",
      "Joe_Wilson_(American_politician) : 72\n",
      "Rob_Bishop : 68\n",
      "Marsha_Blackburn : 67\n",
      "Michael_C._Burgess : 68\n",
      "John_Carter_(Texas_politician) : 78\n",
      "Tom_Cole : 70\n",
      "Mario_Díaz-Balart : 70\n",
      "Trent_Franks : 62\n",
      "Raúl_Grijalva : 62\n",
      "Jeb_Hensarling : 62\n",
      "Steve_King : 70\n",
      "Tim_Murphy_(American_politician) : 67\n",
      "Devin_Nunes : 46\n",
      "Mike_Rogers_(Alabama_politician) : 61\n",
      "Dutch_Ruppersberger : 73\n",
      "Tim_Ryan_(Ohio_politician) : 46\n",
      "Linda_Sánchez : 46\n",
      "David_Scott_(Georgia_politician) : 74\n",
      "Mike_Turner : 59\n",
      "G._K._Butterfield : 72\n",
      "Emanuel_Cleaver : 75\n",
      "Mike_Conaway : 71\n",
      "Jim_Costa : 67\n",
      "Henry_Cuellar : 64\n",
      "Charlie_Dent : 59\n",
      "Jeff_Fortenberry : 58\n",
      "Virginia_Foxx : 76\n",
      "Louie_Gohmert : 66\n",
      "Al_Green_(politician) : 72\n",
      "Brian_Higgins : 60\n",
      "Dan_Lipinski : 53\n",
      "Kenny_Marchant : 68\n",
      "Michael_McCaul : 57\n",
      "Patrick_McHenry : 44\n",
      "Cathy_McMorris_Rodgers : 50\n",
      "Gwen_Moore : 68\n",
      "Ted_Poe : 71\n",
      "Tom_Price_(American_politician) : 65\n",
      "Dave_Reichert : 69\n",
      "Debbie_Wasserman_Schultz : 53\n",
      "Doris_Matsui : 75\n",
      "Albio_Sires : 68\n",
      "Steve_Pearce_(politician) : 72\n",
      "Gus_Bilirakis : 56\n",
      "Vern_Buchanan : 68\n",
      "Kathy_Castor : 53\n",
      "Yvette_Clarke : 55\n",
      "Steve_Cohen : 70\n",
      "Joe_Courtney_(politician) : 66\n",
      "Keith_Ellison : 56\n",
      "Hank_Johnson : 65\n",
      "Jim_Jordan_(American_politician) : 55\n",
      "Doug_Lamborn : 65\n",
      "Dave_Loebsack : 66\n",
      "Kevin_McCarthy_(U.S._Representative) : 54\n",
      "Jerry_McNerney : 68\n",
      "Ed_Perlmutter : 66\n",
      "Peter_Roskam : 58\n",
      "John_Sarbanes : 57\n",
      "Adrian_Smith_(politician) : 48\n",
      "Tim_Walz : 55\n",
      "Peter_Welch : 72\n",
      "John_Yarmuth : 72\n",
      "Niki_Tsongas : 73\n",
      "Bob_Latta : 63\n",
      "Rob_Wittman : 60\n",
      "André_Carson : 60\n",
      "Jackie_Speier : 69\n",
      "Steve_Scalise : 54\n",
      "Marcia_Fudge : 67\n",
      "Rick_Nolan : 75\n",
      "Mark_Sanford : 59\n",
      "Jason_Chaffetz : 52\n",
      "Mike_Coffman : 64\n",
      "Gerry_Connolly : 69\n",
      "Brett_Guthrie : 55\n",
      "Gregg_Harper : 63\n",
      "Jim_Himes : 53\n",
      "Duncan_D._Hunter : 43\n",
      "Lynn_Jenkins : 56\n",
      "Leonard_Lance : 67\n",
      "Blaine_Luetkemeyer : 67\n",
      "Ben_Ray_Luján : 67\n",
      "Tom_McClintock : 63\n",
      "Pete_Olson : 57\n",
      "Erik_Paulsen : 54\n",
      "Chellie_Pingree : 64\n",
      "Jared_Polis : 44\n",
      "Bill_Posey : 71\n",
      "Phil_Roe_(politician) : 74\n",
      "Tom_Rooney_(Florida_politician) : 49\n",
      "Kurt_Schrader : 68\n",
      "Glenn_Thompson_(politician) : 60\n",
      "Paul_Tonko : 70\n",
      "Mike_Quigley_(politician) : 61\n",
      "Judy_Chu : 66\n",
      "John_Garamendi : 74\n",
      "Ted_Deutch : 53\n",
      "Tom_Graves : 49\n",
      "Tom_Reed_(politician) : 48\n",
      "Tim_Walberg : 68\n",
      "Bill_Foster_(politician) : 64\n",
      "Justin_Amash : 39\n",
      "Lou_Barletta : 63\n",
      "Karen_Bass : 66\n",
      "Diane_Black : 68\n",
      "Mo_Brooks : 65\n",
      "Larry_Bucshon : 57\n",
      "David_Cicilline : 58\n",
      "Rick_Crawford_(politician) : 53\n",
      "Jeff_Denham : 52\n",
      "Scott_DesJarlais : 55\n",
      "Sean_Duffy : 48\n",
      "Jeff_Duncan_(politician) : 53\n",
      "Blake_Farenthold : 57\n",
      "Chuck_Fleischmann : 57\n",
      "Bill_Flores : 65\n",
      "Bob_Gibbs : 65\n",
      "Paul_Gosar : 61\n",
      "Trey_Gowdy : 55\n",
      "Morgan_Griffith : 61\n",
      "Andy_Harris_(politician) : 62\n",
      "Vicky_Hartzler : 59\n",
      "Jaime_Herrera_Beutler : 41\n",
      "Bill_Huizenga : 50\n",
      "Randy_Hultgren : 53\n",
      "Bill_Johnson_(Ohio_politician) : 65\n",
      "Bill_Keating_(politician) : 67\n",
      "Mike_Kelly_(Pennsylvania_politician) : 71\n",
      "Adam_Kinzinger : 41\n",
      "Raúl_Labrador : 41\n",
      "Billy_Long : 64\n",
      "Tom_Marino : 67\n",
      "David_McKinley : 72\n",
      "Pat_Meehan : 64\n",
      "Mick_Mulvaney : 52\n",
      "Kristi_Noem : 48\n",
      "Steven_Palazzo : 49\n",
      "Mike_Pompeo : 55\n",
      "Jim_Renacci : 61\n",
      "Cedric_Richmond : 61\n",
      "Martha_Roby : 43\n",
      "Todd_Rokita : 49\n",
      "Dennis_Ross_(politician) : 60\n",
      "David_Schweikert : 57\n",
      "Austin_Scott_(politician) : 50\n",
      "Terri_Sewell : 54\n",
      "Steve_Stivers : 54\n",
      "Scott_Tipton : 63\n",
      "Daniel_Webster_(Florida_politician) : 70\n",
      "Frederica_Wilson : 77\n",
      "Steve_Womack : 62\n",
      "Rob_Woodall : 49\n",
      "Kevin_Yoder : 43\n",
      "Mark_Amodei : 61\n",
      "Suzanne_Bonamici : 65\n",
      "Suzan_DelBene : 57\n",
      "Thomas_Massie : 48\n",
      "Donald_Payne_Jr. : 60\n",
      "Dina_Titus : 69\n",
      "Carol_Shea-Porter : 67\n",
      "Colleen_Hanabusa : 68\n",
      "Andy_Barr_(American_politician) : 46\n",
      "Joyce_Beatty : 69\n",
      "Ami_Bera : 54\n",
      "Jim_Bridenstine : 44\n",
      "Susan_Brooks : 59\n",
      "Julia_Brownley : 67\n",
      "Cheri_Bustos : 58\n",
      "Tony_Cárdenas : 58\n",
      "Matt_Cartwright : 58\n",
      "Joaquín_Castro : 58\n",
      "Chris_Collins_(American_politician) : 69\n",
      "Doug_Collins_(politician) : 53\n",
      "Paul_Cook_(politician) : 76\n",
      "Kevin_Cramer : 58\n",
      "Rodney_Davis_(politician) : 49\n",
      "John_Delaney_(Maryland_politician) : 56\n",
      "Ron_DeSantis : 56\n",
      "Elizabeth_Esty : 60\n",
      "Lois_Frankel : 71\n",
      "Tulsi_Gabbard : 38\n",
      "Denny_Heck : 38\n",
      "George_Holding : 51\n",
      "Richard_Hudson_(American_politician) : 48\n",
      "Jared_Huffman : 55\n",
      "Hakeem_Jeffries : 49\n",
      "David_Joyce_(politician) : 62\n",
      "Joe_Kennedy_III : 39\n",
      "Dan_Kildee : 61\n",
      "Derek_Kilmer : 45\n",
      "Ann_McLane_Kuster : 63\n",
      "Doug_LaMalfa : 59\n",
      "Alan_Lowenthal : 78\n",
      "Michelle_Lujan_Grisham : 60\n",
      "Sean_Patrick_Maloney : 53\n",
      "Mark_Meadows_(North_Carolina_politician) : 60\n",
      "Grace_Meng : 44\n",
      "Luke_Messer : 50\n",
      "Markwayne_Mullin : 42\n",
      "Beto_O'Rourke : 47\n",
      "Scott_Perry_(politician) : 57\n",
      "Scott_Peters_(politician) : 61\n",
      "Robert_Pittenger : 71\n",
      "Mark_Pocan : 55\n",
      "Tom_Rice : 62\n",
      "Keith_Rothfus : 57\n",
      "Raul_Ruiz_(politician) : 47\n",
      "Kyrsten_Sinema : 43\n",
      "Chris_Stewart_(politician) : 59\n",
      "Eric_Swalwell : 39\n",
      "Mark_Takano : 59\n",
      "David_Valadao : 42\n",
      "Juan_Vargas : 58\n",
      "Marc_Veasey : 48\n",
      "Filemon_Vela_Jr. : 56\n",
      "Ann_Wagner : 57\n",
      "Jackie_Walorski : 56\n",
      "Randy_Weber : 66\n",
      "Brad_Wenstrup : 61\n",
      "Roger_Williams_(American_politician) : 70\n",
      "Ted_Yoho : 64\n",
      "Robin_Kelly : 63\n",
      "Jason_Smith_(politician) : 39\n",
      "Katherine_Clark : 56\n",
      "Bradley_Byrne : 64\n",
      "Alma_Adams : 73\n",
      "Dave_Brat : 55\n",
      "Donald_Norcross : 60\n",
      "Ralph_Abraham_(politician) : 65\n",
      "Pete_Aguilar : 40\n",
      "Rick_W._Allen : 68\n",
      "Brian_Babin : 71\n",
      "Don_Beyer : 69\n",
      "Mike_Bishop_(politician) : 52\n",
      "Rod_Blum : 64\n",
      "Mike_Bost : 58\n",
      "Brendan_Boyle : 42\n",
      "Ken_Buck : 60\n",
      "Buddy_Carter : 62\n",
      "Barbara_Comstock : 60\n",
      "Ryan_Costello : 43\n",
      "Carlos_Curbelo : 39\n",
      "Mark_DeSaulnier : 67\n",
      "Debbie_Dingell : 66\n",
      "Tom_Emmer : 58\n",
      "Ruben_Gallego : 58\n",
      "Garret_Graves : 47\n",
      "Glenn_Grothman : 64\n",
      "Jody_Hice : 59\n",
      "French_Hill_(politician) : 63\n",
      "Will_Hurd : 42\n",
      "Evan_Jenkins_(politician) : 59\n",
      "John_Katko : 57\n",
      "Steve_Knight_(politician) : 52\n",
      "Brenda_Lawrence : 65\n",
      "Ted_Lieu : 50\n",
      "Barry_Loudermilk : 55\n",
      "Mia_Love : 44\n",
      "Tom_MacArthur : 59\n",
      "Martha_McSally : 53\n",
      "John_Moolenaar : 58\n",
      "Alex_Mooney : 48\n",
      "Seth_Moulton : 41\n",
      "Dan_Newhouse : 64\n",
      "Gary_Palmer_(politician) : 65\n",
      "Bruce_Poliquin : 66\n",
      "John_Ratcliffe_(American_politician) : 54\n",
      "Kathleen_Rice : 54\n",
      "David_Rouzer : 47\n",
      "Steve_Russell_(politician) : 56\n",
      "Elise_Stefanik : 35\n",
      "Norma_Torres : 54\n",
      "Dave_Trott_(politician) : 59\n",
      "Mark_Walker_(North_Carolina_politician) : 50\n",
      "Mimi_Walters : 57\n",
      "Bonnie_Watson_Coleman : 74\n",
      "Bruce_Westerman : 52\n",
      "David_Young_(Iowa_politician) : 51\n",
      "Lee_Zeldin : 39\n",
      "Ryan_Zinke : 58\n",
      "Dan_Donovan_(politician) : 63\n",
      "Trent_Kelly : 53\n",
      "Darin_LaHood : 51\n",
      "Warren_Davidson : 49\n",
      "James_Comer_(politician) : 47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dwight_Evans_(politician) : 65\n",
      "Brad_Schneider : 58\n",
      "Jodey_Arrington : 47\n",
      "Don_Bacon_(politician) : 56\n",
      "Jim_Banks : 40\n",
      "Nanette_Barragán : 40\n",
      "Jack_Bergman : 72\n",
      "Andy_Biggs : 60\n",
      "Lisa_Blunt_Rochester : 57\n",
      "Anthony_G._Brown : 58\n",
      "Ted_Budd : 48\n",
      "Salud_Carbajal : 55\n",
      "Liz_Cheney : 53\n",
      "Lou_Correa : 61\n",
      "Charlie_Crist : 63\n",
      "Val_Demings : 62\n",
      "Neal_Dunn : 66\n",
      "Adriano_Espaillat : 65\n",
      "John_Faso : 67\n",
      "Drew_Ferguson_(politician) : 53\n",
      "Brian_Fitzpatrick_(American_politician) : 45\n",
      "Matt_Gaetz : 37\n",
      "Mike_Gallagher_(American_politician) : 35\n",
      "Tom_Garrett_(Virginia_politician) : 47\n",
      "Vicente_González_(politician) : 47\n",
      "Josh_Gottheimer : 44\n",
      "Clay_Higgins : 58\n",
      "Trey_Hollingsworth : 36\n",
      "Pramila_Jayapal : 54\n",
      "Mike_Johnson_(Louisiana_politician) : 47\n",
      "Ro_Khanna : 43\n",
      "Ruben_Kihuen : 39\n",
      "Raja_Krishnamoorthi : 46\n",
      "David_Kustoff : 53\n",
      "Al_Lawson : 71\n",
      "Jason_Lewis_(Minnesota_politician) : 64\n",
      "Roger_Marshall_(politician) : 59\n",
      "Brian_Mast : 39\n",
      "Donald_McEachin : 58\n",
      "Paul_Mitchell_(politician) : 63\n",
      "Stephanie_Murphy : 41\n",
      "Tom_O'Halleran : 73\n",
      "Jimmy_Panetta : 50\n",
      "Jamie_Raskin : 56\n",
      "Francis_Rooney : 66\n",
      "Jacky_Rosen : 62\n",
      "John_Rutherford_(Florida_politician) : 67\n",
      "Lloyd_Smucker : 55\n",
      "Darren_Soto : 41\n",
      "Thomas_Suozzi : 57\n",
      "Scott_Taylor_(politician) : 40\n",
      "Claudia_Tenney : 58\n",
      "Ron_Estes : 63\n",
      "Greg_Gianforte : 58\n",
      "Jimmy_Gomez : 45\n",
      "Karen_Handel : 57\n",
      "Ralph_Norman : 66\n",
      "John_Curtis_(American_politician) : 59\n",
      "Conor_Lamb : 35\n",
      "Debbie_Lesko : 61\n",
      "Michael_Cloud : 44\n",
      "Troy_Balderson : 57\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "with open('H115.csv', 'r') as csvinput:\n",
    "    with open('H115_nodes.csv', 'w') as csvoutput:\n",
    "        reader = csv.reader(csvinput, delimiter = ',')\n",
    "        next(csvinput, None)  # skip the header row\n",
    "        writer = csv.writer(csvoutput)\n",
    "        writer.writerow(['WikiPageName', 'Party', 'State', 'Sex', 'Age']) # write the header\n",
    "        \n",
    "        for row in reader:\n",
    "            #print(row)\n",
    "            #print(len(row))\n",
    "            if len(row) == 4: \n",
    "                wikipage = row[0]\n",
    "                res = parsefeature(getinfobox(wikipage), 'bd')\n",
    "                #print(res)\n",
    "                bornyear = int(res[0][0])\n",
    "                bornmonth = int(res[0][1])\n",
    "                bornday = int(res[0][2])\n",
    "                age = calculate_age(bornyear, bornmonth, bornday)\n",
    "                row.append(age)\n",
    "            print(row[0], ':', age) # debugging \n",
    "            writer.writerow(row)\n",
    "            #print(row) \n",
    "            #break\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter data: Edges\n",
    "### Find connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "from datetime import date\n",
    "from itertools import islice\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './tweets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make connections based on retweets\n",
    "# undirected network assumed, so if one person mentions the other, then they are both connected to each other\n",
    "\n",
    "connections = []\n",
    "#count = 0\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    #print(files)\n",
    "    for filename in files:\n",
    "        #print(filename)\n",
    "        for line in open(data_dir + filename, \"r\").readlines():\n",
    "            if line.strip(): # if line is not empty\n",
    "                #print(line)\n",
    "                match = re.search(r'RT @(\\w+)', line)\n",
    "                if match:\n",
    "                    #print(\"match!\")\n",
    "                    cited_handle = match.group(1) # get the retweet handle\n",
    "                    #print(cited_handle)\n",
    "                    if cited_handle in files: # if the retweeted handle belongs to a member of congress\n",
    "                        #print(\"match:\", cited_handle)\n",
    "                        if (filename, cited_handle) or (cited_handle, filename) not in connections: # and the connection between the retweeter and retweeted people has not already been recorded\n",
    "                            connections.append((filename, cited_handle)) # append tuple with (retweeter, retweeted) handles\n",
    "                #break\n",
    "        #break\n",
    "        #count = count + 1\n",
    "        #if count == 10:\n",
    "            #break\n",
    "#connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2773 connections!\n"
     ]
    }
   ],
   "source": [
    "print(\"found\", len(connections), \"connections!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MarkAmodeiNV2', 'SteveScalise')\n"
     ]
    }
   ],
   "source": [
    "# print a connection\n",
    "for pair in connections:\n",
    "    print(pair)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dictionary to translate from Twitter id to Wikipedia id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translates twitter handles to wikipage names based on a list of CURRENT legislators\n",
    "# beware that any current legislator might not have been in the 115th congress\n",
    "\n",
    "translate_dict = dict()\n",
    "\n",
    "with open('legislators-current.csv') as csv_current:\n",
    "        csvReader = csv.reader(csv_current)\n",
    "        next(csvReader, None)  # skip the header row\n",
    "        for row in csvReader:\n",
    "            twitterhandle = row[18]\n",
    "            wikipageid = row[33]\n",
    "            wikipageid = wikipageid.replace(' ', '_')\n",
    "            translate_dict[twitterhandle] = wikipageid\n",
    "            \n",
    "len(translate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not find wiki for: cathymcmorris\n",
      "could not find wiki for: SpeakerRyan\n",
      "could not find wiki for: RepEdRoyce\n",
      "could not find wiki for: RosLehtinen\n",
      "could not find wiki for: RepSeanDuffy\n",
      "could not find wiki for: repgregwalden\n",
      "could not find wiki for: RepDaveBrat\n",
      "could not find wiki for: chelliepingree\n",
      "could not find wiki for: WhipHoyer\n",
      "could not find wiki for: RepCummings\n",
      "could not find wiki for: RepRickAllen\n",
      "could not find wiki for: RepDonBacon\n",
      "could not find wiki for: SamsPressShop\n",
      "could not find wiki for: RepRaskin\n",
      "could not find wiki for: RepRaulGrijalva\n",
      "could not find wiki for: RepDavidKustoff\n",
      "could not find wiki for: RepMcSally\n",
      "could not find wiki for: RepLukeMesser\n",
      "could not find wiki for: repmarkpocan\n",
      "could not find wiki for: RepHensarling\n",
      "could not find wiki for: HurdOnTheHill\n",
      "could not find wiki for: CongressmanHice\n",
      "could not find wiki for: RepCharlieCrist\n",
      "could not find wiki for: RepTenney\n",
      "could not find wiki for: RepBrianMast\n",
      "could not find wiki for: RepCurbelo\n",
      "could not find wiki for: repjohnlewis\n",
      "could not find wiki for: RepDianeBlack\n",
      "could not find wiki for: NancyPelosi\n",
      "could not find wiki for: RepEsty\n",
      "could not find wiki for: rosadelauro\n",
      "could not find wiki for: repjoecrowley\n",
      "could not find wiki for: RepTimWalz\n",
      "could not find wiki for: PeteSessions\n",
      "could not find wiki for: RepJohnDelaney\n",
      "could not find wiki for: USRepRickNolan\n",
      "could not find wiki for: RepRodBlum\n",
      "could not find wiki for: RepOHalleran\n",
      "could not find wiki for: repcleaver\n",
      "could not find wiki for: nikiinthehouse\n",
      "could not find wiki for: sethmoulton\n",
      "could not find wiki for: repblumenauer\n",
      "could not find wiki for: Clyburn\n",
      "could not find wiki for: RepLouCorrea\n",
      "could not find wiki for: KYComer\n",
      "could not find wiki for: RepGoodlatte\n",
      "could not find wiki for: davereichert\n",
      "could not find wiki for: GreggHarper\n",
      "could not find wiki for: RepChrisCollins\n",
      "could not find wiki for: RepJBridenstine\n",
      "could not find wiki for: LamarSmithTX21\n",
      "could not find wiki for: justinamash\n",
      "could not find wiki for: virginiafoxx\n",
      "could not find wiki for: RepSheaPorter\n",
      "could not find wiki for: RepPoliquin\n",
      "could not find wiki for: RepJackyRosen\n",
      "could not find wiki for: RepBrianFitz\n",
      "could not find wiki for: RepMiaLove\n",
      "could not find wiki for: janschakowsky\n",
      "could not find wiki for: RepComstock\n",
      "could not find wiki for: repdinatitus\n",
      "could not find wiki for: RepMimiWalters\n",
      "could not find wiki for: RepBillShuster\n",
      "could not find wiki for: RepLouBarletta\n",
      "could not find wiki for: RepMikeCoffman\n",
      "could not find wiki for: JudgeTedPoe\n",
      "could not find wiki for: RepTomGarrett\n",
      "could not find wiki for: repsandylevin\n",
      "could not find wiki for: RepErikPaulsen\n",
      "could not find wiki for: RepRohrabacher\n",
      "could not find wiki for: RepBrady\n",
      "could not find wiki for: PeterRoskam\n",
      "could not find wiki for: TomRooney\n",
      "could not find wiki for: RepSchneider\n",
      "could not find wiki for: RepJeffDenham\n",
      "could not find wiki for: SteveKnight25\n",
      "could not find wiki for: RepMikeBishop\n",
      "could not find wiki for: DWStweets\n",
      "could not find wiki for: RepStevePearce\n",
      "could not find wiki for: RepRyanCostello\n",
      "could not find wiki for: daveloebsack\n",
      "could not find wiki for: RepBrendanBoyle\n",
      "could not find wiki for: RepKevinYoder\n",
      "could not find wiki for: RepTomMarino\n",
      "could not find wiki for: CongressmanRaja\n",
      "could not find wiki for: RepDeSantis\n",
      "could not find wiki for: jahimes\n",
      "could not find wiki for: boblatta\n",
      "could not find wiki for: RepSinema\n",
      "could not find wiki for: rep_stevewomack\n",
      "could not find wiki for: DarrellIssa\n",
      "could not find wiki for: RepScottTaylor\n",
      "could not find wiki for: davidcicilline\n",
      "could not find wiki for: TGowdySC\n",
      "could not find wiki for: repdonyoung\n",
      "could not find wiki for: RepDanDonovan\n",
      "could not find wiki for: Raul_Labrador\n",
      "could not find wiki for: RepJasonLewis\n",
      "could not find wiki for: replouiegohmert\n",
      "could not find wiki for: RepHanabusa\n",
      "could not find wiki for: RepLujanGrisham\n",
      "could not find wiki for: RepJimRenacci\n",
      "could not find wiki for: repjimcooper\n",
      "could not find wiki for: RepGeneGreen\n",
      "could not find wiki for: DrNealDunnFL2\n",
      "could not find wiki for: RepMikeCapuano\n",
      "could not find wiki for: PatTiberi\n",
      "could not find wiki for: RepTomMacArthur\n",
      "could not find wiki for: RepDavidYoung\n",
      "could not find wiki for: RepJaredPolis\n",
      "could not find wiki for: LacyClayMO1\n",
      "could not find wiki for: RepPeteAguilar\n",
      "could not find wiki for: USRepRodney\n",
      "could not find wiki for: louiseslaughter\n",
      "could not find wiki for: RepKHandel\n",
      "could not find wiki for: RepKristiNoem\n",
      "could not find wiki for: RepBetoORourke\n",
      "could not find wiki for: RepKihuen\n",
      "could not find wiki for: RepDaveTrott\n",
      "could not find wiki for: repdavidscott\n",
      "could not find wiki for: RepTrey\n",
      "could not find wiki for: RepSanfordSC\n",
      "could not find wiki for: RepLanceNJ7\n",
      "could not find wiki for: RepRussell\n",
      "could not find wiki for: congbillposey\n",
      "could not find wiki for: RepDavidValadao\n",
      "could not find wiki for: RepJohnConyers\n",
      "could not find wiki for: RepDennisRoss\n",
      "could not find wiki for: RepLoBiondo\n",
      "could not find wiki for: CongCulberson\n",
      "could not find wiki for: farenthold\n",
      "could not find wiki for: gracenapolitano\n",
      "could not find wiki for: RepPittenger\n",
      "could not find wiki for: keithellison\n",
      "could not find wiki for: RepJoeBarton\n",
      "could not find wiki for: michaelcburgess\n"
     ]
    }
   ],
   "source": [
    "connections_renamed = list()\n",
    "not_translated = list() # keep track of which handles were not translated\n",
    "\n",
    "for pair in connections:\n",
    "    retweeter = pair[0]\n",
    "    retweeted = pair[1]\n",
    "    \n",
    "    retweeter_wiki = ''\n",
    "    if retweeter in translate_dict:\n",
    "        retweeter_wiki = translate_dict[retweeter] \n",
    "    else:\n",
    "        if retweeter not in not_translated:\n",
    "            print(\"could not find wiki for:\", retweeter)\n",
    "            not_translated.append(retweeter)\n",
    "    \n",
    "    retweeted_wiki = ''\n",
    "    if retweeted in translate_dict:\n",
    "        retweeted_wiki = translate_dict[retweeted]\n",
    "    else:\n",
    "        if retweeted not in not_translated:\n",
    "            print(\"could not find wiki for:\", retweeted)\n",
    "            not_translated.append(retweeted)\n",
    "        \n",
    "    if retweeter_wiki != '' and retweeted_wiki != '':\n",
    "        connections_renamed.append((retweeter_wiki,retweeted_wiki))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "managed to rename 1298 connections out of 2773 total connections\n"
     ]
    }
   ],
   "source": [
    "print(\"managed to rename\", len(connections_renamed), \"connections out of\", len(connections), \"total connections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136 twitters could not be translated based on legislators-current.csv\n"
     ]
    }
   ],
   "source": [
    "print(len(set(not_translated)), \"twitters could not be translated based on legislators-current.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to search the `H115_nodes.csv` file for wikipagenames by trying to find longest matching part between twitterhandle and wikipagename.\n",
    "\n",
    "This does not guarantee all are found correctly. Needs manual review and correction after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find common substring between two strings\n",
    "def longestSubstringFinder(string1, string2):\n",
    "    answer = \"\"\n",
    "    len1, len2 = len(string1), len(string2)\n",
    "    for i in range(len1):\n",
    "        match = \"\"\n",
    "        for j in range(len2):\n",
    "            if (i + j < len1 and string1[i + j] == string2[j]):\n",
    "                match += string2[j]\n",
    "            else:\n",
    "                if (len(match) > len(answer)): answer = match\n",
    "                match = \"\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to search the csv file for a matching wikipagename based on a twitterhandle\n",
    "def searchCsvForWiki(twitterhandle):\n",
    "    with open('H115_nodes.csv') as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "        next(csvReader, None)  # skip the header row\n",
    "        tmp = ''\n",
    "        for row in csvReader:\n",
    "            wikipage_untouched = row[0]\n",
    "            wikipage = wikipage_untouched.replace('(_politician)','')#.replace('_','')\n",
    "            # check for match based on forward matching\n",
    "            res = longestSubstringFinder(twitterhandle,wikipage)\n",
    "            if len(res) > len(tmp):\n",
    "                out = wikipage_untouched\n",
    "                tmp = res\n",
    "            # check for match based on backward matching\n",
    "            res = longestSubstringFinder(twitterhandle[::-1], wikipage[::-1])\n",
    "            res = res[::-1]\n",
    "            if len(res) > len(tmp):\n",
    "                out = wikipage_untouched\n",
    "                tmp = res\n",
    "            #if out != '':\n",
    "                #print('match:', out)\n",
    "\n",
    "        #print(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_translate_dict = dict()\n",
    "for name in set(not_translated):\n",
    "    name_cleaned = name.replace('Cong','').replace('Congressman','').replace('Rep','').replace('USRep','')\n",
    "    \n",
    "    manual_translate_dict[name] = searchCsvForWiki(name_cleaned) \n",
    "\n",
    "#manual_translate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "managed to translate 136 twitterhandles out of the remaining 136 untranslated twitterhandles\n"
     ]
    }
   ],
   "source": [
    "print(\"managed to translate\", len(manual_translate_dict), \"twitterhandles out of the remaining\", len(set(not_translated)), \"untranslated twitterhandles\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually correcting wrong translations - based on googling\n",
    "manual_translate_dict['ScottTaylor']='Scott_Taylor_(politician)'\n",
    "manual_translate_dict['MikeBishop']='Mike_Bishop_(politician)'\n",
    "manual_translate_dict['Russell']='Steve_Russell_(politician)'\n",
    "manual_translate_dict['DWStweets']='Debbie_Wasserman_Schultz'\n",
    "manual_translate_dict['DonBacon']='Don_Bacon_(politician)'\n",
    "manual_translate_dict['nikiinthehouse']='Niki_Tsongas'\n",
    "manual_translate_dict['DrNealDunnFL2']='Neal_Dunn'\n",
    "manual_translate_dict['DanDonovan']='Dan_Donovan_(politician)'\n",
    "manual_translate_dict['DaveBrat']='Dave_Brat'\n",
    "manual_translate_dict['JohnDelaney']='John_Delaney_(Maryland_politician)'\n",
    "manual_translate_dict['KYComer']='James_Comer_(politician)'\n",
    "manual_translate_dict['TomGarrett']='Tom_Garrett_(Virginia_politician)'\n",
    "manual_translate_dict['StevePearce']='Steve_Pearce_(politician)'\n",
    "manual_translate_dict['SanfordSC']='Mark_Sanford'\n",
    "manual_translate_dict['BrianMast']='Brian_Mast'\n",
    "manual_translate_dict['HurdOnTheHill']='Will_Hurd'\n",
    "manual_translate_dict['DavidYoung']='David_Young_(Iowa_politician)'\n",
    "manual_translate_dict['DaveTrott']='Dave_Trott_(politician)'\n",
    "manual_translate_dict['JudgeTedPoe']='Ted_Poe'\n",
    "manual_translate_dict['BrianFitz']='Brian_Fitzpatrick_(American_politician)'\n",
    "manual_translate_dict['SteveKnight25']='Steve_Knight_(politician)'\n",
    "manual_translate_dict['boblatta']='Bob_Latta'\n",
    "manual_translate_dict['Brady']='Bob_Brady'\n",
    "manual_translate_dict['repdavidscott']='David_Scott_(Georgia_politician)'\n",
    "manual_translate_dict['CharlieCrist']='Charlie_Crist'\n",
    "manual_translate_dict['KevinYoder']='Kevin_Yoder'\n",
    "manual_translate_dict['KevinYoder']='Kevin_Yoder'\n",
    "translate_dict['GOPLeader'] = 'Kevin_McCarthy_(U.S._Representative)' # correct outdated info from the legislators-current.csv file\n",
    "translate_dict['USRepMikeDoyle'] = 'Mike_Doyle_(American_politician)' # correct outdated info from the legislators-current.csv file\n",
    "translate_dict['RepJerryNadler'] = 'Jerry_Nadler' # correct outdated info from the legislators-current.csv file\n",
    "translate_dict['RepTimRyan'] = 'Tim_Ryan_(Ohio_politician)' # correct outdated info from the legislators-current.csv file\n",
    "#manual_translate_dict['RepEdRoyce']='Ed_Royce' # ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(manual_translate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "683"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_dict = dict(translate_dict)\n",
    "complete_dict.update(manual_translate_dict)\n",
    "len(complete_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate Twitter to Wikipedia and export to csv\n",
    "At this point all twitter handles have been translated. Now we will translate the connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections_renamed = list()\n",
    "\n",
    "for pair in connections:\n",
    "    retweeter = pair[0]\n",
    "    retweeted = pair[1]\n",
    "    \n",
    "    retweeter_wiki = complete_dict[retweeter]\n",
    "    retweeted_wiki = complete_dict[retweeted]\n",
    "    connections_renamed.append((retweeter_wiki, retweeted_wiki))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that we have same number of renamed connections as original connections\n",
    "assert len(connections_renamed) == len(connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write list of connection tuples to csv\n",
    "with open('H115_connections.csv','w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['Retweeter','Retweeted'])\n",
    "    for row in connections_renamed:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot and integer encoding\n",
    "\n",
    "The processed data consists of two files: `H115_nodes.csv` and `H115_connections.csv`. \n",
    "\n",
    "`H115_nodes.csv` is a simple table with the following format:\n",
    "\n",
    "* WikiPageName (string) (id)\n",
    "* Party (string) (label/target)\n",
    "* State (string) (feature)\n",
    "* Sex (string) (feature)\n",
    "* Age (int) (feature)\n",
    "\n",
    "Here, `WikiPageName` is the row ID, and the rest are features. `Party` in the feature we wish to predict is this problem, so this will be the label/target.\n",
    "\n",
    "In order for the NSL library to work with the data, the features must be converted to readable formats. To do this we do the following:\n",
    "\n",
    "* `State` will be converted to a single one-hot vector of length 50.\n",
    "\n",
    "* `Sex` will be converted to a one-hot vector of length 2.\n",
    "\n",
    "The NSL library will automatically process the target labels, so these do *not* need to be converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file and store content\n",
    "features_wiki = []\n",
    "features_state = []\n",
    "features_sex = []\n",
    "features_age = []\n",
    "features_party = []\n",
    "\n",
    "with open('H115_nodes.csv', 'r') as content:\n",
    "    next(content) # skip header\n",
    "    for line in content:\n",
    "        entries = line.rstrip('\\n').split(',')\n",
    "        #print(entries)\n",
    "        features_wiki.append(entries[0])\n",
    "        features_party.append(entries[1])\n",
    "        features_state.append(entries[2])\n",
    "        features_sex.append(entries[3])\n",
    "        features_age.append(entries[4])\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(features_state) == len(features_sex) == len(features_age) == len(features_wiki) == len(features_party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dict of wikipages to turn them into integer IDs\n",
    "values = [i+1 for i in range(len(features_wiki))]\n",
    "wiki_dict = dict(zip(features_wiki, values)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "def onehotencode(l):\n",
    "    l = np.array(l).reshape(-1, 1) # reshape to proper format\n",
    "    cat = OneHotEncoder()\n",
    "    #X = np.array([['a', 'b', 'a', 'c'], [0, 1, 0, 1]], dtype=object).T\n",
    "    onehotvectors = cat.fit_transform(l).toarray()\n",
    "    return onehotvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/anaconda37/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding of each attribute in the feature lists\n",
    "features_state_oh = onehotencode(features_state)\n",
    "features_sex_oh = onehotencode(features_sex)\n",
    "features_age_oh = onehotencode(features_age)\n",
    "features_party_oh = onehotencode(features_party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we expect a one-hot vector length of 50, because there are 50 states\n",
    "len(features_state_oh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we expect a one-hot vector length of 2, because there are 2 genders\n",
    "len(features_sex_oh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many unique age points do we have?\n",
    "len(features_age_oh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we expect two parties\n",
    "len(features_party_oh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assimilate features\n",
    "features_oh = list()\n",
    "for i,v in enumerate(features_wiki):\n",
    "    #features_oh.append([v, features_state_oh[i], features_sex_oh[i], features_age[i], features_party[i]]) #WITHOUT age one-hot encoded \n",
    "    features_oh.append([wiki_dict[v], features_state_oh[i], features_sex_oh[i], features_age_oh[i], features_party[i]]) #WITH age one-hot encoded \n",
    "    #features_oh.append([v, features_state_oh[i], features_sex_oh[i], features_age_oh[i], features_party_oh[i]]) #WITH age one-hot encoded and WITH target (party) onehot encoded\n",
    "    #features_oh.append([v, features_state_oh[i], features_sex_oh[i], features_age[i], features_party_oh[i]]) #WITHOUT age one-hot encoded and WITH target (party) onehot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 1.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1.]), 'Democratic']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample\n",
    "features_oh[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output to tab separated file\n",
    "with open('H115_nodes_oh.tsv', 'w') as content:\n",
    "    #content.write('WikiPageName\\tState\\tSex\\tAge\\tParty\\n')\n",
    "    for line in features_oh:\n",
    "        content.write(str(line[0])+'\\t'+\n",
    "                      str(line[1].astype(np.int)).replace('\\n','').replace('[','').replace(']','').replace(' ','\\t')+'\\t'+\n",
    "                      str(line[2].astype(np.int)).replace('\\n','').replace('[','').replace(']','').replace(' ','\\t')+'\\t'+\n",
    "                      #str(line[3])+'\\t'+ # age WITHOUT onehot encoding\n",
    "                      str(line[3].astype(np.int)).replace('\\n','').replace('[','').replace(']','').replace(' ','\\t')+'\\t'+ # age WITH onehot encoding\n",
    "                      line[4]+'\\n')\n",
    "                      #str(line[4].astype(np.int)).replace('\\n','').replace('[','').replace(']','').replace(' ','\\t')+'\\n') # party WITH onehot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the connections file to a tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('H115_connections.csv', 'r') as infile:\n",
    "    with open('H115_connections.tsv', 'w') as outfile:\n",
    "        next(infile) # skip header\n",
    "        for line in infile:\n",
    "            entries = line.rstrip('\\n').split(',')\n",
    "            #print(entries)\n",
    "            #outfile.write(entries[0]+'\\t'+entries[1]+'\\n') # with actual wiki names\n",
    "            outfile.write(str(wiki_dict[entries[0]])+'\\t'+str(wiki_dict[entries[1]])+'\\n') # with int encoded wiki IDs\n",
    "            #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
